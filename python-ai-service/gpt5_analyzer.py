#!/usr/bin/env python3
"""
GPT-5 Screenplay Analyzer - Writing Excellence Focus
Advanced writing analysis with adaptive reasoning for professional screenplay evaluation
"""

import os
import json
import time
import logging
import uuid
from typing import Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import httpx
from dotenv import load_dotenv
from budget_utils import format_budget_context_for_ai, estimate_budget_from_screenplay, get_casting_suggestions_by_budget

load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GPT5Result:
    """GPT-5 analysis result with writing excellence focus"""
    # Core metrics
    score: float  # 0-10 overall writing quality
    recommendation: str  # Professional recommendation
    executive_assessment: str  # Quick + deep unified assessment
    
    # Writing Excellence Analysis
    character_voice_analysis: Optional[Dict[str, Any]] = None
    dialogue_authenticity: Optional[Dict[str, Any]] = None
    prose_quality: Optional[Dict[str, Any]] = None
    emotional_beat_mapping: Optional[Dict[str, Any]] = None
    
    # Professional Standards
    professional_markers: Optional[Dict[str, Any]] = None
    amateur_indicators: Optional[Dict[str, Any]] = None
    industry_comparison: Optional[Dict[str, Any]] = None
    
    # Adaptive reasoning metadata
    reasoning_depth: str = "auto"  # "quick", "deep", "hybrid"
    reasoning_tokens: int = 0
    
    # Processing metadata
    processing_time: float = 0.0
    cost: float = 0.0
    confidence: float = 0.0
    raw_response: str = ""

class GPT5Analyzer:
    """GPT-5 API integration for advanced screenplay writing analysis"""
    
    def __init__(self):
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.api_url = "https://api.openai.com/v1/chat/completions"
        self.model = "gpt-5-chat-latest"  # Latest GPT-5 model
        
        # GPT-5 pricing (estimated - adjust based on actual pricing)
        self.input_cost_per_token = 0.00002  # $0.02 per 1K tokens
        self.output_cost_per_token = 0.00008  # $0.08 per 1K tokens
        self.reasoning_cost_per_token = 0.00012  # $0.12 per 1K reasoning tokens
        
        if not self.api_key:
            logger.warning("âš ï¸  OPENAI_API_KEY not set - GPT-5 analysis will be disabled")
        else:
            logger.info("ðŸ§  GPT-5 Writing Excellence Analyzer initialized")
    
    async def analyze(self, screenplay_text: str, title: str, genre: str = "", budget_estimate: Optional[float] = None) -> Optional[GPT5Result]:
        """Analyze screenplay with GPT-5 writing excellence focus"""
        
        if not self.api_key:
            logger.warning("âŒ GPT-5 analysis skipped - no API key")
            return None
            
        try:
            start_time = time.time()
            
            # Create adaptive writing excellence prompt
            prompt = self._create_writing_excellence_prompt(screenplay_text, title, genre, budget_estimate)
            
            # Call GPT-5 API with adaptive reasoning
            response = await self._call_gpt5_api(prompt)
            
            # Parse response with writing excellence focus
            analysis_data = self._parse_response(response)
            
            # Calculate cost including reasoning tokens
            input_tokens = self._estimate_tokens(prompt)
            output_tokens = self._estimate_tokens(response)
            reasoning_tokens = analysis_data.get('_reasoning_tokens', 0)
            
            cost = (
                (input_tokens * self.input_cost_per_token) + 
                (output_tokens * self.output_cost_per_token) +
                (reasoning_tokens * self.reasoning_cost_per_token)
            )
            
            processing_time = time.time() - start_time
            
            # Calculate dynamic score if not provided
            raw_score = analysis_data.get('score')
            if raw_score is None or raw_score == 0:
                calculated_score = self._calculate_dynamic_gpt5_score(analysis_data)
                logger.info(f"ðŸ§  GPT-5 score calculated dynamically: {calculated_score:.1f}/10")
            else:
                calculated_score = float(raw_score)

            result = GPT5Result(
                score=calculated_score,
                recommendation=analysis_data.get('recommendation', self._get_gpt5_recommendation_from_score(calculated_score)),
                executive_assessment=analysis_data.get('executive_assessment', ''),
                character_voice_analysis=analysis_data.get('character_voice_analysis'),
                dialogue_authenticity=analysis_data.get('dialogue_authenticity'),
                prose_quality=analysis_data.get('prose_quality'),
                emotional_beat_mapping=analysis_data.get('emotional_beat_mapping'),
                professional_markers=analysis_data.get('professional_markers'),
                amateur_indicators=analysis_data.get('amateur_indicators'),
                industry_comparison=analysis_data.get('industry_comparison'),
                reasoning_depth=analysis_data.get('reasoning_depth', 'auto'),
                reasoning_tokens=reasoning_tokens,
                processing_time=processing_time,
                cost=cost,
                confidence=analysis_data.get('confidence', 0.8),
                raw_response=response
            )
            
            logger.info(f"ðŸ§  GPT-5 analysis completed in {processing_time:.2f}s")
            logger.info(f"ðŸ’° GPT-5 cost: ${cost:.4f} (reasoning: {reasoning_tokens} tokens)")
            logger.info(f"â­ GPT-5 score: {result.score}/10 ({result.recommendation})")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ GPT-5 analysis failed: {e}")
            return None
    
    def _create_writing_excellence_prompt(self, screenplay_text: str, title: str, genre: str, budget_estimate: Optional[float] = None) -> str:
        """Create adaptive writing excellence prompt for GPT-5"""
        
        # Truncate if too long (GPT-5 has large context but we want efficiency)
        max_chars = 200000  # Use substantial context for full analysis
        if len(screenplay_text) > max_chars:
            screenplay_text = screenplay_text[:max_chars] + "\n\n[TRUNCATED FOR ANALYSIS]"
        
        # Enhanced budget context with AI estimation for writing analysis
        estimated_budget = None
        if not budget_estimate:
            estimated_budget = estimate_budget_from_screenplay(screenplay_text, title, genre or "Drama")
        
        budget_context = format_budget_context_for_ai(budget_estimate, estimated_budget)
        
        # Get budget-appropriate casting considerations for writing assessment
        budget_for_casting = budget_estimate or estimated_budget[1] if estimated_budget else 10_000_000
        casting_strategy = get_casting_suggestions_by_budget(budget_for_casting, genre or "Drama")

        return f"""You are a top-tier Hollywood script reader and development executive with 20+ years of experience evaluating screenplays for major studios. You have an exceptional eye for writing quality and can distinguish professional from amateur work instantly.

**SCREENPLAY: "{title}"**
{f"**GENRE: {genre}**" if genre else ""}
{budget_context}

**SCREENPLAY TEXT:**
{screenplay_text}

**ADAPTIVE ANALYSIS INSTRUCTIONS:**

Use your adaptive reasoning capabilities to provide both quick professional impressions AND deep analysis where complex elements require nuanced understanding. Think deeply about writing craft elements that separate professional from amateur work.

**ANALYSIS FRAMEWORK:**

## **EXECUTIVE ASSESSMENT** (Quick + Deep Unified)
Provide immediate professional impressions, then engage deeper reasoning for complex writing elements:

**QUICK SCAN:**
- Format and structure compliance
- Obvious writing strengths/weaknesses  
- Genre execution basics
- Immediate red flags or standout elements

**DEEP ANALYSIS** (engage reasoning for complex elements):
- Character psychology authenticity in dialogue
- Subtext layers and emotional undercurrents
- Professional vs amateur writing markers
- Thematic integration subtlety

## **WRITING EXCELLENCE CATEGORIES:**

### **CHARACTER VOICE ANALYSIS**
Think deeply about character distinction and authenticity:
- Does each character speak with a unique, authentic voice?
- Can you identify characters by dialogue alone (without action lines)?
- Are character voices consistent throughout the script?
- Do voices match character backgrounds, education, emotional states?
- Identify any dialogue that feels interchangeable between characters

### **DIALOGUE AUTHENTICITY**
Analyze dialogue realism and purpose:
- Does dialogue sound natural when spoken aloud?
- Is exposition handled organically or forced?
- Are there authentic speech patterns, interruptions, subtext?
- Does dialogue serve multiple purposes (character, plot, theme)?
- Identify any "on-the-nose" or overly expository dialogue

### **PROSE QUALITY** 
Evaluate action lines and visual storytelling:
- Are action lines concise, visual, and filmable?
- Does the script "show" rather than "tell"?
- Is the writing style appropriate for the genre?
- Are scene descriptions engaging and cinematic?
- Does the prose maintain proper pacing and white space?

### **EMOTIONAL BEAT MAPPING**
Trace emotional trajectory and authenticity:
- Does each scene have clear emotional purpose?
- Are emotional beats earned through proper setup?
- Is there authentic emotional progression throughout?
- Do characters react believably to emotional situations?
- Are emotional payoffs satisfying and well-prepared?

### **PROFESSIONAL STANDARDS ASSESSMENT**
Compare against industry benchmarks:
- What separates this from amateur writing?
- Which elements meet professional standards?
- What would immediately flag this as amateur work?
- How does this compare to successful scripts in the genre?
- What would need improvement for studio consideration?

**RESPONSE FORMAT (JSON):**
```json
{{
    "score": 0.0,
    "recommendation": "",
    "confidence": 0.0,
    "reasoning_depth": "auto",
    "executive_assessment": {{
        "quick_impressions": "",
        "deep_analysis": "",
        "professional_verdict": ""
    }},
    "character_voice_analysis": {{
        "voice_distinction_score": 0,
        "authenticity_rating": "",
        "character_voice_examples": [],
        "interchangeable_dialogue_issues": [],
        "voice_consistency_assessment": ""
    }},
    "dialogue_authenticity": {{
        "naturalism_score": 0,
        "exposition_handling": "",
        "subtext_quality": "",
        "speech_pattern_authenticity": "",
        "on_the_nose_examples": []
    }},
    "prose_quality": {{
        "visual_storytelling_score": 0,
        "action_line_efficiency": "",
        "cinematic_language": "",
        "pacing_through_prose": "",
        "show_vs_tell_balance": ""
    }},
    "emotional_beat_mapping": {{
        "emotional_arc_clarity": 0,
        "beat_authenticity": "",
        "setup_payoff_effectiveness": "",
        "character_reaction_believability": "",
        "emotional_progression_analysis": ""
    }},
    "professional_markers": {{
        "industry_standard_elements": [],
        "professional_craft_indicators": [],
        "studio_ready_aspects": []
    }},
    "amateur_indicators": {{
        "red_flag_elements": [],
        "craft_improvement_needs": [],
        "amateur_writing_patterns": []
    }},
    "industry_comparison": {{
        "comparable_professional_scripts": [],
        "competitive_positioning": "",
        "market_readiness_assessment": ""
    }}
}}
```

**ANALYSIS APPROACH:**
- Use adaptive reasoning to match analysis depth to complexity
- Engage deep thinking for nuanced writing craft elements
- Provide specific examples from the script
- Focus on actionable professional insights
- Maintain industry-standard evaluation criteria

Think deeply about elements that require professional expertise to evaluate properly."""

    async def _call_gpt5_api(self, prompt: str) -> str:
        """Call GPT-5 API with adaptive reasoning enabled"""
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "You are a top-tier Hollywood script reader and development executive with exceptional writing analysis capabilities. Use adaptive reasoning to provide both quick professional assessments and deep analysis for complex writing craft elements."
                },
                {
                    "role": "user", 
                    "content": prompt
                }
            ],
            "max_completion_tokens": 4000,  # Increased for comprehensive writing analysis
            "temperature": 1.0,  # GPT-5 only supports default temperature
            "stream": False
        }
        
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(
                self.api_url,
                headers=headers,
                json=payload
            )
            
            if response.status_code != 200:
                raise Exception(f"GPT-5 API error: {response.status_code} - {response.text}")
            
            data = response.json()
            
            if 'choices' not in data or not data['choices']:
                raise Exception("Invalid GPT-5 API response format")
            
            return data['choices'][0]['message']['content']
    
    def _parse_response(self, response: str) -> Dict[str, Any]:
        """Parse GPT-5 response with writing excellence focus"""
        
        try:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{[\s\S]*\}', response)
            
            if json_match:
                json_str = json_match.group()
                
                # Try to fix truncated JSON if needed
                json_str = self._fix_truncated_json(json_str)
                
                parsed_data = json.loads(json_str)
                
                # Validate and structure the response
                return self._validate_response(parsed_data)
            else:
                logger.warning("âš ï¸  No JSON found in GPT-5 response, using fallback parsing")
                return self._fallback_parse(response)
                
        except json.JSONDecodeError as e:
            logger.warning(f"âš ï¸  JSON parsing failed: {e}, attempting to fix")
            try:
                json_match = re.search(r'\{[\s\S]*', response)
                if json_match:
                    json_str = self._fix_truncated_json(json_match.group())
                    parsed_data = json.loads(json_str)
                    logger.info("âœ… Successfully parsed truncated JSON after repair")
                    return self._validate_response(parsed_data)
            except:
                pass
            
            logger.warning("âš ï¸  All JSON parsing attempts failed, using fallback")
            return self._fallback_parse(response)
    
    def _fix_truncated_json(self, json_str: str) -> str:
        """Fix truncated JSON by adding missing closing braces and quotes"""
        
        # Count opening and closing braces/brackets
        open_braces = json_str.count('{')
        close_braces = json_str.count('}')
        open_brackets = json_str.count('[')
        close_brackets = json_str.count(']')
        
        # If JSON is truncated in the middle of a string value, close the string
        if json_str.count('"') % 2 == 1:
            json_str += '"'
        
        # Add missing closing brackets
        while close_brackets < open_brackets:
            json_str += ']'
            close_brackets += 1
        
        # Add missing closing braces
        while close_braces < open_braces:
            json_str += '}'
            close_braces += 1
        
        return json_str
    
    def _validate_response(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Validate and structure GPT-5 writing excellence response"""
        
        # Calculate dynamic score if not provided
        raw_score = data.get('score')
        if raw_score is None or raw_score == 0:
            calculated_score = self._calculate_dynamic_gpt5_score(data)
            logger.info(f"ðŸ§  GPT-5 validation score calculated dynamically: {calculated_score:.1f}/10")
        else:
            calculated_score = float(raw_score)

        # Ensure basic fields exist
        validated = {
            'score': min(10.0, max(0.0, calculated_score)),
            'recommendation': data.get('recommendation', self._get_gpt5_recommendation_from_score(calculated_score)),
            'confidence': min(1.0, max(0.0, data.get('confidence', 0.8))),
            'reasoning_depth': data.get('reasoning_depth', 'auto')
        }
        
        # Validate executive assessment
        if 'executive_assessment' in data:
            exec_assess = data['executive_assessment']
            validated['executive_assessment'] = {
                'quick_impressions': exec_assess.get('quick_impressions', ''),
                'deep_analysis': exec_assess.get('deep_analysis', ''),
                'professional_verdict': exec_assess.get('professional_verdict', '')
            }
        
        # Validate writing excellence categories
        writing_categories = [
            'character_voice_analysis', 'dialogue_authenticity', 
            'prose_quality', 'emotional_beat_mapping',
            'professional_markers', 'amateur_indicators', 'industry_comparison'
        ]
        
        for category in writing_categories:
            if category in data:
                validated[category] = data[category]
        
        return validated
    
    def _fallback_parse(self, response: str) -> Dict[str, Any]:
        """Fallback parsing when JSON extraction fails"""
        
        # Extract score using regex
        import re
        
        score_match = re.search(r'score["\s:]*(\d+\.?\d*)', response, re.IGNORECASE)
        
        if score_match:
            score = float(score_match.group(1))
        else:
            # Use content-based scoring when no explicit score found
            score = self._analyze_gpt5_text_for_score(response)
            logger.info(f"ðŸ§  GPT-5 fallback text analysis score: {score:.1f}/10")
        
        # Determine recommendation based on score
        if score >= 8.5:
            recommendation = "Strong Recommend"
        elif score >= 7.0:
            recommendation = "Recommend"
        elif score >= 5.0:
            recommendation = "Consider"
        else:
            recommendation = "Pass"
        
        # Extract key insights from response
        lines = response.split('\n')
        assessment = next((line.strip() for line in lines if len(line.strip()) > 30), "Analysis completed")
        
        return {
            'score': score,
            'recommendation': recommendation,
            'executive_assessment': assessment[:500],
            'confidence': 0.7,
            'reasoning_depth': 'fallback'
        }
    
    def _estimate_tokens(self, text: str) -> int:
        """Estimate token count for GPT-5"""
        return max(1, len(text) // 4)
    
    def _calculate_dynamic_gpt5_score(self, data: Dict[str, Any]) -> float:
        """Calculate dynamic GPT-5 score based on writing excellence analysis components"""
        
        base_score = 5.5  # Start slightly above middle for GPT-5's writing focus
        
        try:
            # Character Voice Analysis scoring (0-2 points)
            voice_analysis = data.get('character_voice_analysis', {})
            if voice_analysis:
                voice_score = voice_analysis.get('voice_distinction_score', 5)
                if voice_score >= 8:
                    base_score += 1.5
                elif voice_score >= 6:
                    base_score += 1.0
                elif voice_score >= 4:
                    base_score += 0.5
                
                authenticity = voice_analysis.get('authenticity_rating', '').lower()
                if any(word in authenticity for word in ['excellent', 'outstanding', 'exceptional']):
                    base_score += 0.5
            
            # Dialogue Authenticity scoring (0-1.5 points)
            dialogue_auth = data.get('dialogue_authenticity', {})
            if dialogue_auth:
                naturalism = dialogue_auth.get('naturalism_score', 5)
                if naturalism >= 8:
                    base_score += 1.0
                elif naturalism >= 6:
                    base_score += 0.5
                
                subtext = dialogue_auth.get('subtext_quality', '').lower()
                if any(word in subtext for word in ['excellent', 'sophisticated', 'layered']):
                    base_score += 0.5
            
            # Prose Quality scoring (0-1.5 points)
            prose_quality = data.get('prose_quality', {})
            if prose_quality:
                visual_score = prose_quality.get('visual_storytelling_score', 5)
                if visual_score >= 8:
                    base_score += 1.0
                elif visual_score >= 6:
                    base_score += 0.5
                
                cinematic = prose_quality.get('cinematic_language', '').lower()
                if any(word in cinematic for word in ['excellent', 'masterful', 'cinematic']):
                    base_score += 0.5
            
            # Executive Assessment scoring (0-1 points)
            executive = data.get('executive_assessment', '')
            if executive:
                exec_text = executive.lower()
                if any(word in exec_text for word in ['exceptional', 'outstanding', 'masterpiece']):
                    base_score += 1.0
                elif any(word in exec_text for word in ['strong', 'solid', 'well-crafted']):
                    base_score += 0.5
                elif any(word in exec_text for word in ['weak', 'problematic', 'needs work']):
                    base_score -= 0.5
            
            # Add GPT-5 specific randomness for writing craft variation
            import hashlib
            content_hash = int(hashlib.md5(str(data).encode()).hexdigest()[:8], 16)
            import random
            random.seed(content_hash % 2147483647)
            variation = (random.random() - 0.5) * 0.6  # Â±0.3 variation
            base_score += variation
            
            # Ensure score is within bounds
            final_score = max(1.0, min(10.0, base_score))
            
            logger.info(f"ðŸ§  GPT-5 dynamic score: Voice={voice_analysis.get('voice_distinction_score', 'N/A')}, "
                       f"Dialogue={dialogue_auth.get('naturalism_score', 'N/A')}, "
                       f"Prose={prose_quality.get('visual_storytelling_score', 'N/A')} â†’ Score={final_score:.1f}/10")
            
            return final_score
            
        except Exception as e:
            logger.warning(f"âš ï¸  Error in GPT-5 dynamic scoring: {e}, using base score")
            return 5.0  # Middle default for writing excellence

    def _get_gpt5_recommendation_from_score(self, score: float) -> str:
        """Get GPT-5 recommendation based on calculated score"""
        if score >= 8.5:
            return "Strong Recommend"
        elif score >= 7.0:
            return "Recommend"
        elif score >= 5.0:
            return "Consider"
        else:
            return "Pass"

    def _analyze_gpt5_text_for_score(self, text: str) -> float:
        """Analyze GPT-5 response text to estimate a score when no explicit score is provided"""
        
        text_lower = text.lower()
        base_score = 5.0  # Start at middle for writing excellence
        
        # GPT-5 writing excellence positive indicators
        positive_words = [
            'exceptional', 'outstanding', 'masterful', 'brilliant', 'sophisticated',
            'well-crafted', 'compelling', 'authentic', 'nuanced', 'layered',
            'cinematic', 'professional', 'polished', 'distinctive voice'
        ]
        
        # GPT-5 writing craft negative indicators
        negative_words = [
            'weak', 'amateur', 'clunky', 'stilted', 'on-the-nose',
            'heavy-handed', 'clichÃ©d', 'predictable', 'underdeveloped',
            'lacks depth', 'needs work', 'problematic dialogue'
        ]
        
        # Writing craft specific indicators
        craft_positive = [
            'strong character voice', 'authentic dialogue', 'visual storytelling',
            'subtext', 'character development', 'emotional depth',
            'cinematic language', 'show don\'t tell'
        ]
        
        craft_negative = [
            'exposition heavy', 'talking heads', 'flat characters',
            'wooden dialogue', 'lacks subtext', 'tells not shows'
        ]
        
        # Count indicators
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        craft_pos = sum(1 for phrase in craft_positive if phrase in text_lower)
        craft_neg = sum(1 for phrase in craft_negative if phrase in text_lower)
        
        # Adjust score based on indicators
        base_score += (positive_count * 0.4) - (negative_count * 0.5)
        base_score += (craft_pos * 0.3) - (craft_neg * 0.4)
        
        # Look for GPT-5 style recommendations
        if any(phrase in text_lower for phrase in ['writing excellence', 'masterful craft']):
            base_score = max(base_score, 8.5)
        elif any(phrase in text_lower for phrase in ['strong writing', 'well-executed']):
            base_score = max(base_score, 7.0)
        elif any(phrase in text_lower for phrase in ['needs significant work', 'major issues']):
            base_score = min(base_score, 4.0)
        
        # Add GPT-5 specific randomness
        import hashlib
        text_hash = int(hashlib.md5(text.encode()).hexdigest()[:8], 16)
        import random
        random.seed(text_hash % 2147483647)
        variation = (random.random() - 0.5) * 0.5  # Â±0.25 variation
        base_score += variation
        
        # Ensure bounds
        return max(1.0, min(10.0, base_score))
    
    def to_database_format(self, result: GPT5Result) -> Dict[str, Any]:
        """Convert GPT-5 result to database format"""
        
        db_data = {
            'gpt5_score': result.score,
            'gpt5_recommendation': result.recommendation,
            'gpt5_executive_assessment': result.executive_assessment,
            'gpt5_reasoning_depth': result.reasoning_depth,
            'gpt5_reasoning_tokens': result.reasoning_tokens,
            'gpt5_confidence': result.confidence,
            'gpt5_processing_time': result.processing_time,
            'gpt5_cost': result.cost,
            'gpt5_raw_response': result.raw_response
        }
        
        # Add writing excellence analysis as JSON
        if result.character_voice_analysis:
            db_data['gpt5_character_voice_analysis'] = json.dumps(result.character_voice_analysis)
        if result.dialogue_authenticity:
            db_data['gpt5_dialogue_authenticity'] = json.dumps(result.dialogue_authenticity)
        if result.prose_quality:
            db_data['gpt5_prose_quality'] = json.dumps(result.prose_quality)
        if result.emotional_beat_mapping:
            db_data['gpt5_emotional_beat_mapping'] = json.dumps(result.emotional_beat_mapping)
        if result.professional_markers:
            db_data['gpt5_professional_markers'] = json.dumps(result.professional_markers)
        if result.amateur_indicators:
            db_data['gpt5_amateur_indicators'] = json.dumps(result.amateur_indicators)
        if result.industry_comparison:
            db_data['gpt5_industry_comparison'] = json.dumps(result.industry_comparison)
        
        return db_data
